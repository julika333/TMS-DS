{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/julika333/TMS-DS/blob/main/TMS_nb/Lesson_27.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32958786-e08e-4a7a-bc54-b38106d7bf54",
      "metadata": {
        "id": "32958786-e08e-4a7a-bc54-b38106d7bf54"
      },
      "source": [
        "# Установка библиотек:\n",
        "Убедитесь, что у вас установлены библиотеки hyperopt и tensorflow. Если нет, установите их с помощью команды:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "237c0c44-4697-4c4a-b656-3c247fc3d723",
      "metadata": {
        "id": "237c0c44-4697-4c4a-b656-3c247fc3d723",
        "outputId": "b632567b-ea56-4e91-f515-5211091faa4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting hyperopt\n",
            "  Using cached hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
            "Requirement already satisfied: tensorflow in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (2.13.0)\n",
            "Requirement already satisfied: numpy in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from hyperopt) (1.23.5)\n",
            "Requirement already satisfied: scipy in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from hyperopt) (1.11.1)\n",
            "Requirement already satisfied: six in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from hyperopt) (1.16.0)\n",
            "Requirement already satisfied: networkx>=2.2 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from hyperopt) (3.1)\n",
            "Collecting future (from hyperopt)\n",
            "  Using cached future-0.18.3.tar.gz (840 kB)\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: tqdm in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from hyperopt) (4.66.0)\n",
            "Collecting cloudpickle (from hyperopt)\n",
            "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
            "Collecting py4j (from hyperopt)\n",
            "  Using cached py4j-0.10.9.7-py2.py3-none-any.whl (200 kB)\n",
            "Requirement already satisfied: tensorflow-intel==2.13.0 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow) (2.13.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.1.21 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (16.0.6)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (4.23.4)\n",
            "Requirement already satisfied: setuptools in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (68.0.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.3.0)\n",
            "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)\n",
            "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.15.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.56.2)\n",
            "Requirement already satisfied: tensorboard<2.14,>=2.13 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.14,>=2.13.0 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.0)\n",
            "Requirement already satisfied: keras<2.14,>=2.13.1 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (2.13.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (0.31.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from tqdm->hyperopt) (0.4.6)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.41.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.22.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (4.9)\n",
            "Requirement already satisfied: urllib3<2.0 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.16)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2023.7.22)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in c:\\users\\sidromnik\\.conda\\envs\\tensorflow\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\sidromnik\\appdata\\roaming\\python\\python311\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
            "Building wheels for collected packages: future\n",
            "  Building wheel for future (setup.py): started\n",
            "  Building wheel for future (setup.py): finished with status 'done'\n",
            "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492053 sha256=9176799bf0b730d852c86a8758feae140d5fac5e36b73b455943a606c0d20ec2\n",
            "  Stored in directory: c:\\users\\sidromnik\\appdata\\local\\pip\\cache\\wheels\\da\\19\\ca\\9d8c44cd311a955509d7e13da3f0bea42400c469ef825b580b\n",
            "Successfully built future\n",
            "Installing collected packages: py4j, typing-extensions, future, cloudpickle, hyperopt\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.7.1\n",
            "    Uninstalling typing_extensions-4.7.1:\n",
            "      Successfully uninstalled typing_extensions-4.7.1\n",
            "Successfully installed cloudpickle-2.2.1 future-0.18.3 hyperopt-0.2.7 py4j-0.10.9.7 typing-extensions-4.5.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "pydantic 2.1.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.4.0 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n"
          ]
        }
      ],
      "source": [
        "!pip install hyperopt tensorflow\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3da91f09-b983-469b-b683-7c9b7df2364d",
      "metadata": {
        "id": "3da91f09-b983-469b-b683-7c9b7df2364d"
      },
      "source": [
        "Импорт библиотек:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ddd9ce7-cd55-4d44-8ab5-c968ac7b50d1",
      "metadata": {
        "id": "9ddd9ce7-cd55-4d44-8ab5-c968ac7b50d1"
      },
      "outputs": [],
      "source": [
        "import hyperopt\n",
        "from hyperopt import fmin, tpe, hp\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import Dense, Flatten\n",
        "from tensorflow.keras.models import Sequential\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8f9753d-9b79-4bfb-85d7-3e129da89b46",
      "metadata": {
        "id": "d8f9753d-9b79-4bfb-85d7-3e129da89b46"
      },
      "source": [
        "Загрузка данных:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e54b53f9-e5d1-4143-98c3-ecc9382f6353",
      "metadata": {
        "id": "e54b53f9-e5d1-4143-98c3-ecc9382f6353"
      },
      "outputs": [],
      "source": [
        "# Загрузка и предобработка данных\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7234619e-3294-4c1a-8851-3a47ffdcc5ca",
      "metadata": {
        "id": "7234619e-3294-4c1a-8851-3a47ffdcc5ca"
      },
      "source": [
        "Определение функции для оптимизации:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7e013c6-b6f9-48f6-b30c-ca95bb82965e",
      "metadata": {
        "id": "d7e013c6-b6f9-48f6-b30c-ca95bb82965e"
      },
      "outputs": [],
      "source": [
        "def objective(params):\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(32, 32, 3)),\n",
        "        Dense(params['units'], activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=params['learning_rate']),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), verbose=0)\n",
        "    val_accuracy = history.history['val_accuracy'][-1]\n",
        "\n",
        "    return -val_accuracy  # Минимизация, так как Hyperopt ищет минимум функции\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f309b12c-6405-4192-8681-d1ffbcb8276d",
      "metadata": {
        "id": "f309b12c-6405-4192-8681-d1ffbcb8276d"
      },
      "source": [
        "Определение пространства поиска:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c7ee752-b2c4-4f9d-a32c-9506eb8cc998",
      "metadata": {
        "id": "8c7ee752-b2c4-4f9d-a32c-9506eb8cc998"
      },
      "outputs": [],
      "source": [
        "space = {\n",
        "    'units': hp.choice('units', [32, 64, 128]),  # Количество нейронов в слое\n",
        "    'learning_rate': hp.loguniform('learning_rate', -5, -1)  # Логарифмический выбор скорости обучения\n",
        "}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47b078ea-6aa9-4833-bb82-b33b2b1de882",
      "metadata": {
        "id": "47b078ea-6aa9-4833-bb82-b33b2b1de882"
      },
      "source": [
        "Запуск поиска:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0611af3-6935-41be-8ca5-bbe6407a943e",
      "metadata": {
        "id": "c0611af3-6935-41be-8ca5-bbe6407a943e",
        "outputId": "3424b0ba-b6f8-43e0-817a-b2008261ee18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [07:06<00:00, 42.62s/trial, best loss: -0.17319999635219574]\n"
          ]
        }
      ],
      "source": [
        "best = fmin(fn=objective,\n",
        "            space=space,\n",
        "            algo=tpe.suggest,\n",
        "            max_evals=10)  # Количество экспериментов\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb925f2b-aeb4-4b8f-8c7d-88b7fed990bf",
      "metadata": {
        "id": "eb925f2b-aeb4-4b8f-8c7d-88b7fed990bf"
      },
      "source": [
        "В этом примере мы использовали библиотеку Hyperopt для оптимизации гиперпараметров нейронной сети для задачи классификации CIFAR-10 с использованием TensorFlow. Мы определили функцию objective, которую Hyperopt будет минимизировать, и пространство поиска гиперпараметров. Затем мы запустили поиск с помощью алгоритма tpe.suggest и ограничили количество экспериментов до 10."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "100fca41-a1aa-4212-98ca-03a8728296b1",
      "metadata": {
        "id": "100fca41-a1aa-4212-98ca-03a8728296b1"
      },
      "source": [
        "примеры использования библиотеки Scikit-learn для поиска гиперпараметров нейронной сети на задаче классификации изображений CIFAR-10 с использованием TensorFlow:\n",
        "\n",
        "\n",
        "## Grid Search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c189adb5-ae52-4511-b760-45440cc9c72e",
      "metadata": {
        "id": "c189adb5-ae52-4511-b760-45440cc9c72e"
      },
      "outputs": [],
      "source": [
        "# !pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5fb1c891-02dd-48b8-962a-7b6137e1676a",
      "metadata": {
        "id": "5fb1c891-02dd-48b8-962a-7b6137e1676a",
        "outputId": "fa3f9864-8d0f-47f1-d953-02b3b8eb283e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 1ms/step - loss: 0.6358\n",
            "32/32 [==============================] - 0s 839us/step\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from tensorflow import keras\n",
        "\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "\n",
        "X, y = make_classification(1000, 20, n_informative=10, random_state=0)\n",
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.int64)\n",
        "\n",
        "def get_model(hidden_layer_dim, meta):\n",
        "    # note that meta is a special argument that will be\n",
        "    # handed a dict containing input metadata\n",
        "    n_features_in_ = meta[\"n_features_in_\"]\n",
        "    X_shape_ = meta[\"X_shape_\"]\n",
        "    n_classes_ = meta[\"n_classes_\"]\n",
        "\n",
        "    model = keras.models.Sequential()\n",
        "    model.add(keras.layers.Dense(n_features_in_, input_shape=X_shape_[1:]))\n",
        "    model.add(keras.layers.Activation(\"relu\"))\n",
        "    model.add(keras.layers.Dense(hidden_layer_dim))\n",
        "    model.add(keras.layers.Activation(\"relu\"))\n",
        "    model.add(keras.layers.Dense(n_classes_))\n",
        "    model.add(keras.layers.Activation(\"softmax\"))\n",
        "    return model\n",
        "\n",
        "clf = KerasClassifier(\n",
        "    get_model,\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    hidden_layer_dim=100,\n",
        ")\n",
        "\n",
        "clf.fit(X, y)\n",
        "y_proba = clf.predict_proba(X)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93e48f8a-2720-4561-8c35-44847d112d78",
      "metadata": {
        "id": "93e48f8a-2720-4561-8c35-44847d112d78",
        "outputId": "bf6fe31c-dfbd-4fde-9e36-da634b09b614"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "32/32 [==============================] - 0s 969us/step - loss: 0.6619\n",
            "32/32 [==============================] - 0s 742us/step\n"
          ]
        }
      ],
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "pipe = Pipeline([\n",
        "    ('scale', StandardScaler()),\n",
        "    ('clf', clf),\n",
        "])\n",
        "\n",
        "pipe.fit(X, y)\n",
        "y_proba = pipe.predict_proba(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae6e55b8-0fa2-49d7-ac97-80aea9f7d3df",
      "metadata": {
        "scrolled": true,
        "id": "ae6e55b8-0fa2-49d7-ac97-80aea9f7d3df",
        "outputId": "7e581d85-22c6-4feb-dfa1-74fcb246a3e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 0s 1ms/step - loss: 0.8182\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 1.0400\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 980us/step - loss: 0.7523\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.7825\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.7421\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 1s 1000us/step - loss: 0.7173\n",
            "11/11 [==============================] - 0s 898us/step\n",
            "21/21 [==============================] - 1s 999us/step - loss: 0.8326\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.9327\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 1s 1ms/step - loss: 0.8435\n",
            "11/11 [==============================] - 0s 898us/step\n",
            "21/21 [==============================] - 0s 951us/step - loss: 1.3553\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.8270\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.7955\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.8581\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 1.1878\n",
            "11/11 [==============================] - 0s 803us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.6730\n",
            "11/11 [==============================] - 0s 897us/step\n",
            "21/21 [==============================] - 0s 900us/step - loss: 0.6464\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.5954\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.6343\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1000us/step - loss: 0.8075\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1000us/step - loss: 0.9556\n",
            "11/11 [==============================] - 0s 903us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.9257\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.6463\n",
            "11/11 [==============================] - 0s 897us/step\n",
            "21/21 [==============================] - 0s 998us/step - loss: 0.8684\n",
            "11/11 [==============================] - 0s 903us/step\n",
            "21/21 [==============================] - 1s 1ms/step - loss: 0.6520\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 1s 1ms/step - loss: 1.1100\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1000us/step - loss: 1.2007\n",
            "11/11 [==============================] - 0s 803us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 1.2638\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1000us/step - loss: 0.6704\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.7134\n",
            "11/11 [==============================] - 0s 802us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.8297\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.7295\n",
            "11/11 [==============================] - 0s 893us/step\n",
            "21/21 [==============================] - 0s 945us/step - loss: 0.8261\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.8322\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.5658\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.6203\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.5597\n",
            "11/11 [==============================] - 0s 798us/step\n",
            "21/21 [==============================] - 1s 1000us/step - loss: 0.7251\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.7127\n",
            "11/11 [==============================] - 0s 799us/step\n",
            "21/21 [==============================] - 0s 993us/step - loss: 0.9270\n",
            "11/11 [==============================] - 0s 700us/step\n",
            "21/21 [==============================] - 0s 999us/step - loss: 0.6278\n",
            "11/11 [==============================] - 0s 801us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.6550\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 999us/step - loss: 0.6228\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 1.4711\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 1s 950us/step - loss: 2.2532\n",
            "11/11 [==============================] - 0s 803us/step\n",
            "21/21 [==============================] - 1s 1ms/step - loss: 1.5133\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.7715\n",
            "11/11 [==============================] - 0s 899us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.6597\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1000us/step - loss: 1.0544\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1000us/step - loss: 0.7645\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1000us/step - loss: 0.7255\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 999us/step - loss: 0.7287\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 955us/step - loss: 0.6148\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.6350\n",
            "11/11 [==============================] - 0s 803us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.6536\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "0.7360114605623588 {'hidden_layer_dim': 100, 'loss': 'sparse_categorical_crossentropy', 'optimizer': 'sgd', 'optimizer__learning_rate': 0.1}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "params = {\n",
        "    \"hidden_layer_dim\": [50, 100, 200],\n",
        "    \"loss\": [\"sparse_categorical_crossentropy\"],\n",
        "    \"optimizer\": [\"adam\", \"sgd\"],\n",
        "    \"optimizer__learning_rate\": [0.0001, 0.001, 0.1],\n",
        "}\n",
        "gs = GridSearchCV(clf, params, refit=False, cv=3, scoring='accuracy')\n",
        "\n",
        "gs.fit(X, y)\n",
        "print(gs.best_score_, gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0206d346-71fb-4b26-9574-e46afb153ad9",
      "metadata": {
        "id": "0206d346-71fb-4b26-9574-e46afb153ad9"
      },
      "source": [
        "## Randomized Search:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44e95019-b6db-4245-b663-8304a17d3d48",
      "metadata": {
        "scrolled": true,
        "id": "44e95019-b6db-4245-b663-8304a17d3d48",
        "outputId": "a16fdb82-ce4b-49de-d9a1-527c94f5e235"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "21/21 [==============================] - 0s 976us/step - loss: 0.7992\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 944us/step - loss: 0.8350\n",
            "11/11 [==============================] - 0s 898us/step\n",
            "21/21 [==============================] - 0s 976us/step - loss: 0.9681\n",
            "11/11 [==============================] - 0s 898us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 1.0783\n",
            "11/11 [==============================] - 0s 803us/step\n",
            "21/21 [==============================] - 0s 949us/step - loss: 1.0207\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 1.0151\n",
            "11/11 [==============================] - 0s 997us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.6144\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1000us/step - loss: 0.5861\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.6322\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.7482\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 2ms/step - loss: 0.8354\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 975us/step - loss: 0.7741\n",
            "11/11 [==============================] - 0s 897us/step\n",
            "21/21 [==============================] - 1s 1ms/step - loss: 0.6989\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.8249\n",
            "11/11 [==============================] - 0s 898us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.7283\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.8983\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.9032\n",
            "11/11 [==============================] - 0s 897us/step\n",
            "21/21 [==============================] - 0s 995us/step - loss: 0.9347\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.9859\n",
            "11/11 [==============================] - 0s 897us/step\n",
            "21/21 [==============================] - 1s 1ms/step - loss: 1.0138\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1000us/step - loss: 0.9777\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.6732\n",
            "11/11 [==============================] - 0s 801us/step\n",
            "21/21 [==============================] - 0s 984us/step - loss: 0.7170\n",
            "11/11 [==============================] - 0s 800us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.7530\n",
            "11/11 [==============================] - 0s 803us/step\n",
            "21/21 [==============================] - 0s 950us/step - loss: 0.8010\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.7758\n",
            "11/11 [==============================] - 0s 900us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 0.8607\n",
            "11/11 [==============================] - 0s 1ms/step\n",
            "21/21 [==============================] - 1s 1ms/step - loss: 2.1126\n",
            "11/11 [==============================] - 0s 897us/step\n",
            "21/21 [==============================] - 0s 1ms/step - loss: 1.4202\n",
            "11/11 [==============================] - 0s 902us/step\n",
            "21/21 [==============================] - 1s 1ms/step - loss: 1.8255\n",
            "11/11 [==============================] - 0s 700us/step\n",
            "0.7410134685583788 {'optimizer__learning_rate': 0.1, 'optimizer': 'adam', 'loss': 'sparse_categorical_crossentropy', 'hidden_layer_dim': 100}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "params = {\n",
        "    \"hidden_layer_dim\": [50, 100, 200],\n",
        "    \"loss\": [\"sparse_categorical_crossentropy\"],\n",
        "    \"optimizer\": [\"adam\", \"sgd\"],\n",
        "    \"optimizer__learning_rate\": [0.0001, 0.001, 0.1],\n",
        "}\n",
        "gs = RandomizedSearchCV(clf, params, refit=False, cv=3, scoring='accuracy')\n",
        "\n",
        "gs.fit(X, y)\n",
        "print(gs.best_score_, gs.best_params_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82af37f5-dc39-46ea-aa60-db7236e67c99",
      "metadata": {
        "id": "82af37f5-dc39-46ea-aa60-db7236e67c99"
      },
      "source": [
        "В этих примерах мы использовали библиотеку Scikit-learn для поиска гиперпараметров нейронной сети для задачи классификации CIFAR-10 с использованием TensorFlow. Мы определили функцию create_model, которая создает модель нейронной сети, и параметры поиска param_dist (для Randomized Search) и param_grid (для Grid Search). Затем мы создали объекты RandomizedSearchCV и GridSearchCV и выполнили поиск с использованием кросс-валидации."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1af9860f-c066-4aae-93d1-1b6f7c1910ae",
      "metadata": {
        "id": "1af9860f-c066-4aae-93d1-1b6f7c1910ae"
      },
      "source": [
        "пример использования библиотеки Optuna для настройки гиперпараметров нейронной сети на задаче классификации изображений CIFAR-10 с использованием TensorFlow:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2a1fd52-df1c-4a40-a32e-bc90ecf65e7e",
      "metadata": {
        "id": "c2a1fd52-df1c-4a40-a32e-bc90ecf65e7e"
      },
      "outputs": [],
      "source": [
        "# !pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "979f1d80-df90-44c2-81b3-53ffcb8f7261",
      "metadata": {
        "id": "979f1d80-df90-44c2-81b3-53ffcb8f7261",
        "outputId": "be48dc59-1e8f-415c-a57d-32a10996daff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2023-08-30 14:23:47,264] A new study created in memory with name: no-name-fec5a293-d11f-44fa-8076-fcb8ce6672fd\n",
            "C:\\Users\\Sidromnik\\AppData\\Local\\Temp\\ipykernel_11752\\3713110107.py:15: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
            "  learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
            "[I 2023-08-30 14:24:14,730] Trial 0 finished with value: 0.3935000002384186 and parameters: {'units': 32, 'learning_rate': 1.7722962087941887e-05}. Best is trial 0 with value: 0.3935000002384186.\n",
            "[I 2023-08-30 14:25:27,520] Trial 1 finished with value: 0.41110000014305115 and parameters: {'units': 128, 'learning_rate': 1.07200310649519e-05}. Best is trial 1 with value: 0.41110000014305115.\n",
            "[I 2023-08-30 14:25:54,069] Trial 2 finished with value: 0.37299999594688416 and parameters: {'units': 32, 'learning_rate': 1.0546417474900653e-05}. Best is trial 1 with value: 0.41110000014305115.\n",
            "[I 2023-08-30 14:26:21,045] Trial 3 finished with value: 0.10000000149011612 and parameters: {'units': 32, 'learning_rate': 0.004293572970622467}. Best is trial 1 with value: 0.41110000014305115.\n",
            "[I 2023-08-30 14:26:56,563] Trial 4 finished with value: 0.43389999866485596 and parameters: {'units': 64, 'learning_rate': 0.000415930267537645}. Best is trial 4 with value: 0.43389999866485596.\n",
            "[I 2023-08-30 14:28:07,386] Trial 5 finished with value: 0.4505000114440918 and parameters: {'units': 128, 'learning_rate': 0.0004399588235447994}. Best is trial 5 with value: 0.4505000114440918.\n",
            "[I 2023-08-30 14:29:16,637] Trial 6 finished with value: 0.4652999937534332 and parameters: {'units': 128, 'learning_rate': 8.665223022938086e-05}. Best is trial 6 with value: 0.4652999937534332.\n",
            "[I 2023-08-30 14:29:42,202] Trial 7 finished with value: 0.257099986076355 and parameters: {'units': 32, 'learning_rate': 0.002274480039480658}. Best is trial 6 with value: 0.4652999937534332.\n",
            "[I 2023-08-30 14:30:13,988] Trial 8 finished with value: 0.415800005197525 and parameters: {'units': 64, 'learning_rate': 1.5105094210000371e-05}. Best is trial 6 with value: 0.4652999937534332.\n",
            "[I 2023-08-30 14:30:37,532] Trial 9 finished with value: 0.4083000123500824 and parameters: {'units': 32, 'learning_rate': 0.00012608062515058108}. Best is trial 6 with value: 0.4652999937534332.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'units': 128, 'learning_rate': 8.665223022938086e-05}\n",
            "Best value found: 46.53%\n"
          ]
        }
      ],
      "source": [
        "import optuna\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# Load and preprocess CIFAR-10 data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Define the objective function\n",
        "def objective(trial):\n",
        "    units = trial.suggest_categorical('units', [32, 64, 128])\n",
        "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-2)\n",
        "\n",
        "    model = Sequential([\n",
        "        Flatten(input_shape=(32, 32, 3)),\n",
        "        Dense(units, activation='relu'),\n",
        "        Dense(10, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer,\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(x_train, y_train, epochs=10, validation_data=(x_test, y_test), verbose=0)\n",
        "    val_accuracy = history.history['val_accuracy'][-1]\n",
        "\n",
        "    return val_accuracy\n",
        "\n",
        "# Create a study object and optimize\n",
        "study = optuna.create_study(direction='maximize')  # 'maximize' for accuracy, 'minimize' for loss\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# Print the best parameters and value\n",
        "print(\"Best parameters found: \", study.best_params)\n",
        "print(\"Best value found: {:.2f}%\".format(study.best_value * 100))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06ae4c5d-8779-4bd7-b8d8-c17d66b918ca",
      "metadata": {
        "id": "06ae4c5d-8779-4bd7-b8d8-c17d66b918ca"
      },
      "outputs": [],
      "source": [
        "# !pip install keras-tuner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c63712da-a292-4c15-b24c-72edd0ea5dcf",
      "metadata": {
        "id": "c63712da-a292-4c15-b24c-72edd0ea5dcf",
        "outputId": "61257c43-98b4-4507-9f8c-606ad9043447"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trial 5 Complete [00h 05m 56s]\n",
            "val_accuracy: 0.6844000220298767\n",
            "\n",
            "Best val_accuracy So Far: 0.7070000171661377\n",
            "Total elapsed time: 00h 20m 44s\n",
            "INFO:tensorflow:Oracle triggered exit\n",
            "Results summary\n",
            "Results in my_dir\\cifar10\n",
            "Showing 10 best trials\n",
            "Objective(name=\"val_accuracy\", direction=\"max\")\n",
            "\n",
            "Trial 0 summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 48\n",
            "conv_1_kernel: 3\n",
            "num_conv_layers: 1\n",
            "conv_2_filter: 32\n",
            "conv_2_kernel: 3\n",
            "dense_units: 288\n",
            "learning_rate: 0.001\n",
            "Score: 0.7070000171661377\n",
            "\n",
            "Trial 4 summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 48\n",
            "conv_1_kernel: 5\n",
            "num_conv_layers: 1\n",
            "conv_2_filter: 80\n",
            "conv_2_kernel: 5\n",
            "dense_units: 256\n",
            "learning_rate: 0.001\n",
            "conv_3_filter: 64\n",
            "conv_3_kernel: 3\n",
            "conv_4_filter: 112\n",
            "conv_4_kernel: 3\n",
            "Score: 0.6844000220298767\n",
            "\n",
            "Trial 2 summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 112\n",
            "conv_1_kernel: 5\n",
            "num_conv_layers: 1\n",
            "conv_2_filter: 96\n",
            "conv_2_kernel: 5\n",
            "dense_units: 224\n",
            "learning_rate: 0.01\n",
            "conv_3_filter: 128\n",
            "conv_3_kernel: 3\n",
            "conv_4_filter: 96\n",
            "conv_4_kernel: 5\n",
            "Score: 0.38199999928474426\n",
            "\n",
            "Trial 1 summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 80\n",
            "conv_1_kernel: 5\n",
            "num_conv_layers: 3\n",
            "conv_2_filter: 64\n",
            "conv_2_kernel: 5\n",
            "dense_units: 96\n",
            "learning_rate: 0.01\n",
            "conv_3_filter: 32\n",
            "conv_3_kernel: 3\n",
            "conv_4_filter: 32\n",
            "conv_4_kernel: 3\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 270, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 235, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 287, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 213, in _build_and_fit_model\n",
            "    model = self._try_build(hp)\n",
            "            ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 155, in _try_build\n",
            "    model = self._build_hypermodel(hp)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 146, in _build_hypermodel\n",
            "    model = self.hypermodel.build(hp)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\AppData\\Local\\Temp\\ipykernel_11752\\1024711899.py\", line 18, in build_model\n",
            "    model.add(Conv2D(filters=hp.Int(f'conv_{i+2}_filter', min_value=32, max_value=128, step=16),\n",
            "  File \"C:\\Users\\Sidromnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\trackable\\base.py\", line 204, in _method_wrapper\n",
            "    result = method(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"C:\\Users\\Sidromnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py\", line 354, in compute_output_shape\n",
            "    raise ValueError(\n",
            "ValueError: One of the dimensions in the output is <= 0 due to downsampling in conv2d_3. Consider increasing the input size. Received input shape [None, 1, 1, 32] which would produce output shape with a zero or negative value in a dimension.\n",
            "\n",
            "\n",
            "Trial 3 summary\n",
            "Hyperparameters:\n",
            "conv_1_filter: 80\n",
            "conv_1_kernel: 5\n",
            "num_conv_layers: 3\n",
            "conv_2_filter: 112\n",
            "conv_2_kernel: 3\n",
            "dense_units: 320\n",
            "learning_rate: 0.001\n",
            "conv_3_filter: 112\n",
            "conv_3_kernel: 3\n",
            "conv_4_filter: 96\n",
            "conv_4_kernel: 3\n",
            "Traceback (most recent call last):\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 270, in _try_run_and_update_trial\n",
            "    self._run_and_update_trial(trial, *fit_args, **fit_kwargs)\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\base_tuner.py\", line 235, in _run_and_update_trial\n",
            "    results = self.run_trial(trial, *fit_args, **fit_kwargs)\n",
            "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 287, in run_trial\n",
            "    obj_value = self._build_and_fit_model(trial, *args, **copied_kwargs)\n",
            "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 213, in _build_and_fit_model\n",
            "    model = self._try_build(hp)\n",
            "            ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 155, in _try_build\n",
            "    model = self._build_hypermodel(hp)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\.conda\\envs\\Tensorflow\\Lib\\site-packages\\keras_tuner\\engine\\tuner.py\", line 146, in _build_hypermodel\n",
            "    model = self.hypermodel.build(hp)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\AppData\\Local\\Temp\\ipykernel_11752\\1024711899.py\", line 18, in build_model\n",
            "    model.add(Conv2D(filters=hp.Int(f'conv_{i+2}_filter', min_value=32, max_value=128, step=16),\n",
            "  File \"C:\\Users\\Sidromnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\trackable\\base.py\", line 204, in _method_wrapper\n",
            "    result = method(self, *args, **kwargs)\n",
            "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"C:\\Users\\Sidromnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n",
            "    raise e.with_traceback(filtered_tb) from None\n",
            "  File \"C:\\Users\\Sidromnik\\AppData\\Roaming\\Python\\Python311\\site-packages\\tensorflow\\python\\framework\\ops.py\", line 1751, in _create_c_op\n",
            "    raise ValueError(e.message)\n",
            "ValueError: Exception encountered when calling layer \"conv2d_3\" (type Conv2D).\n",
            "\n",
            "Negative dimension size caused by subtracting 3 from 2 for '{{node conv2d_3/Conv2D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](Placeholder, conv2d_3/Conv2D/ReadVariableOp)' with input shapes: [?,2,2,112], [3,3,112,96].\n",
            "\n",
            "Call arguments received by layer \"conv2d_3\" (type Conv2D):\n",
            "  • inputs=tf.Tensor(shape=(None, 2, 2, 112), dtype=float32)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from kerastuner.tuners import RandomSearch\n",
        "\n",
        "# Загрузка данных CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Определение модели для настройки гиперпараметров\n",
        "def build_model(hp):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Conv2D(filters=hp.Int('conv_1_filter', min_value=32, max_value=128, step=16),\n",
        "                     kernel_size=hp.Choice('conv_1_kernel', values=[3, 5]),\n",
        "                     activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    for i in range(hp.Int('num_conv_layers', 1, 3)):\n",
        "        model.add(Conv2D(filters=hp.Int(f'conv_{i+2}_filter', min_value=32, max_value=128, step=16),\n",
        "                         kernel_size=hp.Choice(f'conv_{i+2}_kernel', values=[3, 5]),\n",
        "                         activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=hp.Int('dense_units', min_value=32, max_value=512, step=32),\n",
        "                    activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3])),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Создание объекта Keras Tuner RandomSearch\n",
        "tuner = RandomSearch(build_model,\n",
        "                     objective='val_accuracy',\n",
        "                     max_trials=5,  # Количество случайных комбинаций гиперпараметров для попыток\n",
        "                     directory='my_dir',  # Папка для сохранения результатов настройки\n",
        "                     project_name='cifar10')\n",
        "\n",
        "# Поиск оптимальных гиперпараметров\n",
        "tuner.search(x_train, y_train, epochs=10, validation_split=0.1)\n",
        "\n",
        "# Вывод результатов настройки\n",
        "tuner.results_summary()\n",
        "\n",
        "# Получение лучшей модели\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b347833-634b-41e7-baea-3dc0d2318eef",
      "metadata": {
        "id": "0b347833-634b-41e7-baea-3dc0d2318eef"
      },
      "outputs": [],
      "source": [
        "# !pip install bayesian-optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84c39d4a-a4d0-4259-9714-beed79a0ebd8",
      "metadata": {
        "id": "84c39d4a-a4d0-4259-9714-beed79a0ebd8",
        "outputId": "712a9394-61ad-4aea-9fe9-8696f57dc736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "|   iter    |  target   | conv_1... | conv_1... | dense_... | learni... | num_co... |\n",
            "-------------------------------------------------------------------------------------\n",
            "| \u001b[0m1        \u001b[0m | \u001b[0m0.5894   \u001b[0m | \u001b[0m72.03    \u001b[0m | \u001b[0m4.441    \u001b[0m | \u001b[0m32.05    \u001b[0m | \u001b[0m0.003093 \u001b[0m | \u001b[0m1.294    \u001b[0m |\n",
            "| \u001b[0m2        \u001b[0m | \u001b[0m0.5804   \u001b[0m | \u001b[0m40.86    \u001b[0m | \u001b[0m3.373    \u001b[0m | \u001b[0m197.9    \u001b[0m | \u001b[0m0.004028 \u001b[0m | \u001b[0m2.078    \u001b[0m |\n",
            "| \u001b[0m3        \u001b[0m | \u001b[0m0.0986   \u001b[0m | \u001b[0m72.24    \u001b[0m | \u001b[0m4.37     \u001b[0m | \u001b[0m130.1    \u001b[0m | \u001b[0m0.008793 \u001b[0m | \u001b[0m1.055    \u001b[0m |\n",
            "| \u001b[95m4        \u001b[0m | \u001b[95m0.6882   \u001b[0m | \u001b[95m96.36    \u001b[0m | \u001b[95m3.835    \u001b[0m | \u001b[95m300.2    \u001b[0m | \u001b[95m0.00149  \u001b[0m | \u001b[95m1.396    \u001b[0m |\n",
            "| \u001b[0m5        \u001b[0m | \u001b[0m0.0976   \u001b[0m | \u001b[0m108.9    \u001b[0m | \u001b[0m4.937    \u001b[0m | \u001b[0m182.4    \u001b[0m | \u001b[0m0.006954 \u001b[0m | \u001b[0m2.753    \u001b[0m |\n",
            "| \u001b[0m6        \u001b[0m | \u001b[0m0.5696   \u001b[0m | \u001b[0m96.22    \u001b[0m | \u001b[0m4.464    \u001b[0m | \u001b[0m300.5    \u001b[0m | \u001b[0m0.0027   \u001b[0m | \u001b[0m2.041    \u001b[0m |\n",
            "| \u001b[0m7        \u001b[0m | \u001b[0m0.4946   \u001b[0m | \u001b[0m97.73    \u001b[0m | \u001b[0m3.296    \u001b[0m | \u001b[0m298.7    \u001b[0m | \u001b[0m0.007028 \u001b[0m | \u001b[0m1.133    \u001b[0m |\n",
            "| \u001b[0m8        \u001b[0m | \u001b[0m0.097    \u001b[0m | \u001b[0m94.69    \u001b[0m | \u001b[0m3.613    \u001b[0m | \u001b[0m299.9    \u001b[0m | \u001b[0m0.009769 \u001b[0m | \u001b[0m1.268    \u001b[0m |\n",
            "| \u001b[0m9        \u001b[0m | \u001b[0m0.097    \u001b[0m | \u001b[0m96.58    \u001b[0m | \u001b[0m4.594    \u001b[0m | \u001b[0m299.3    \u001b[0m | \u001b[0m0.004438 \u001b[0m | \u001b[0m1.269    \u001b[0m |\n",
            "| \u001b[0m10       \u001b[0m | \u001b[0m0.0976   \u001b[0m | \u001b[0m96.88    \u001b[0m | \u001b[0m3.587    \u001b[0m | \u001b[0m300.0    \u001b[0m | \u001b[0m0.00811  \u001b[0m | \u001b[0m1.415    \u001b[0m |\n",
            "| \u001b[0m11       \u001b[0m | \u001b[0m0.559    \u001b[0m | \u001b[0m54.45    \u001b[0m | \u001b[0m3.275    \u001b[0m | \u001b[0m456.3    \u001b[0m | \u001b[0m0.0001583\u001b[0m | \u001b[0m2.498    \u001b[0m |\n",
            "| \u001b[0m12       \u001b[0m | \u001b[0m0.4426   \u001b[0m | \u001b[0m84.84    \u001b[0m | \u001b[0m3.428    \u001b[0m | \u001b[0m503.7    \u001b[0m | \u001b[0m0.004321 \u001b[0m | \u001b[0m2.65     \u001b[0m |\n",
            "| \u001b[0m13       \u001b[0m | \u001b[0m0.1024   \u001b[0m | \u001b[0m95.49    \u001b[0m | \u001b[0m4.008    \u001b[0m | \u001b[0m157.3    \u001b[0m | \u001b[0m0.006261 \u001b[0m | \u001b[0m1.814    \u001b[0m |\n",
            "| \u001b[0m14       \u001b[0m | \u001b[0m0.4696   \u001b[0m | \u001b[0m45.52    \u001b[0m | \u001b[0m3.534    \u001b[0m | \u001b[0m440.3    \u001b[0m | \u001b[0m0.004545 \u001b[0m | \u001b[0m2.421    \u001b[0m |\n",
            "| \u001b[0m15       \u001b[0m | \u001b[0m0.0976   \u001b[0m | \u001b[0m77.02    \u001b[0m | \u001b[0m3.075    \u001b[0m | \u001b[0m94.51    \u001b[0m | \u001b[0m0.007086 \u001b[0m | \u001b[0m1.937    \u001b[0m |\n",
            "=====================================================================================\n",
            "Best parameters: {'conv_1_filter': 96.36488097712662, 'conv_1_kernel': 3.834609604734254, 'dense_units': 300.1711176539608, 'learning_rate': 0.0014898306920928144, 'num_conv_layers': 1.3962029781697576}\n",
            "Best validation accuracy: 0.6881999969482422\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "from bayes_opt import BayesianOptimization\n",
        "\n",
        "# Загрузка данных CIFAR-10\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# Определение функции для оптимизации\n",
        "def keras_cnn(learning_rate, conv_1_filter, conv_1_kernel,\n",
        "              num_conv_layers, dense_units):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(Conv2D(filters=int(conv_1_filter), kernel_size=int(conv_1_kernel),\n",
        "                     activation='relu', input_shape=(32, 32, 3)))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    for i in range(int(num_conv_layers)):\n",
        "        model.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(units=int(dense_units), activation='relu'))\n",
        "    model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "                  loss='sparse_categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    history = model.fit(x_train, y_train, epochs=5, validation_split=0.1, verbose=0)\n",
        "    val_accuracy = history.history['val_accuracy'][-1]\n",
        "    return val_accuracy\n",
        "\n",
        "# Определение диапазонов гиперпараметров для оптимизации\n",
        "pbounds = {'learning_rate': (1e-4, 1e-2),\n",
        "           'conv_1_filter': (32, 128),\n",
        "           'conv_1_kernel': (3, 5),\n",
        "           'num_conv_layers': (1, 3),\n",
        "           'dense_units': (32, 512)}\n",
        "\n",
        "# Создание объекта BayesianOptimization\n",
        "optimizer = BayesianOptimization(f=keras_cnn, pbounds=pbounds, verbose=2, random_state=1)\n",
        "\n",
        "# Запуск оптимизации\n",
        "optimizer.maximize(init_points=5, n_iter=10)\n",
        "\n",
        "# Вывод результатов оптимизации\n",
        "print(\"Best parameters:\", optimizer.max['params'])\n",
        "print(\"Best validation accuracy:\", optimizer.max['target'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba67abe4-a1a9-44a4-87de-f7b3adc6fa7b",
      "metadata": {
        "id": "ba67abe4-a1a9-44a4-87de-f7b3adc6fa7b"
      },
      "outputs": [],
      "source": [
        "# conda create -n SMAC python=3.10\n",
        "# conda activate SMAC\n",
        "# conda install gxx_linux-64 gcc_linux-64 swig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57b63048-38d4-4c35-b073-c9561d85cd0d",
      "metadata": {
        "id": "57b63048-38d4-4c35-b073-c9561d85cd0d"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# import tensorflow as tf\n",
        "# from tensorflow.keras.datasets import cifar10\n",
        "# from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
        "# from smac.facade.func_facade import fmin_smac\n",
        "\n",
        "# # Загрузка данных CIFAR-10\n",
        "# (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "# x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "# # Определение функции для оптимизации\n",
        "# def keras_cnn(learning_rate, conv_1_filter, conv_1_kernel,\n",
        "#               num_conv_layers, dense_units):\n",
        "#     model = tf.keras.Sequential()\n",
        "#     model.add(Conv2D(filters=int(conv_1_filter), kernel_size=int(conv_1_kernel),\n",
        "#                      activation='relu', input_shape=(32, 32, 3)))\n",
        "#     model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#     for i in range(int(num_conv_layers)):\n",
        "#         model.add(Conv2D(filters=32, kernel_size=3, activation='relu'))\n",
        "#         model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "#     model.add(Flatten())\n",
        "#     model.add(Dense(units=int(dense_units), activation='relu'))\n",
        "#     model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "#     model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate),\n",
        "#                   loss='sparse_categorical_crossentropy',\n",
        "#                   metrics=['accuracy'])\n",
        "\n",
        "#     history = model.fit(x_train, y_train, epochs=5, validation_split=0.1, verbose=0)\n",
        "#     val_accuracy = history.history['val_accuracy'][-1]\n",
        "#     return -val_accuracy\n",
        "\n",
        "# # Определение диапазонов гиперпараметров для оптимизации\n",
        "# param_space = {'learning_rate': ('log-uniform', 1e-4, 1e-2),\n",
        "#                'conv_1_filter': (32, 128),\n",
        "#                'conv_1_kernel': (3, 5),\n",
        "#                'num_conv_layers': (1, 3),\n",
        "#                'dense_units': (32, 512)}\n",
        "\n",
        "# # Запуск оптимизации с использованием SMAC\n",
        "# best_config, _ = fmin_smac(func=keras_cnn,  # Функция для оптимизации\n",
        "#                            x0=[0.001, 64, 3, 2, 128],  # Начальные значения гиперпараметров\n",
        "#                            bounds=[(1e-4, 1e-2), (32, 128), (3, 5), (1, 3), (32, 512)],  # Диапазоны гиперпараметров\n",
        "#                            maxfun=20,  # Количество итераций\n",
        "#                            rng=np.random.RandomState(42))\n",
        "\n",
        "# # Вывод наилучших параметров\n",
        "# print(\"Best parameters:\", best_config)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33a05a44-94a6-4779-af43-0b53e1c89528",
      "metadata": {
        "id": "33a05a44-94a6-4779-af43-0b53e1c89528"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "797b2e7e-f289-4fea-8111-c7af69088968",
      "metadata": {
        "id": "797b2e7e-f289-4fea-8111-c7af69088968"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71794469-8052-46ec-9e0f-eb01e5f632c8",
      "metadata": {
        "id": "71794469-8052-46ec-9e0f-eb01e5f632c8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c935057f-d0fb-4bfc-b211-b830af4b9c50",
      "metadata": {
        "id": "c935057f-d0fb-4bfc-b211-b830af4b9c50"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30ece7fd-f354-4351-adf4-281886df503c",
      "metadata": {
        "id": "30ece7fd-f354-4351-adf4-281886df503c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3593690b-de00-4f5f-8afa-ce72722ab357",
      "metadata": {
        "id": "3593690b-de00-4f5f-8afa-ce72722ab357"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}